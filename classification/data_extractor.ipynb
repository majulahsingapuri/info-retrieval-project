{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 10:28:46.963099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-df29f072a3b1628c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2639bc14354f3e8ddf412450342ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-d0480d9a4aaa7bae/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cbbf257b4d4b9cbab56eaba52773a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292c0d688ca3463bb0e352425d7bd6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-d0480d9a4aaa7bae/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-fe9974d2b3317d75/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16703a399cba4ef78a19f5ae6ee70a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load three datasets: dataset contains all the data without labels, dataset_train_eval contains data with labels\n",
    "# test contains data without labels\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\",  data_files=\"../indexing/output.csv\")\n",
    "# We split the data test:eval 70:30 \n",
    "dataset_train_eval = (load_dataset(\"csv\", data_files=\"./balanced_labels_posneg.csv\", split = \"train\").train_test_split(test_size=0.3))\n",
    "\n",
    "test = load_dataset(\"csv\", data_files=\"./test.csv\")\n",
    "# dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column label to label_fine_grained [0,5]\n",
    "# dataset_train_eval = dataset_train_eval.rename_column(\"label\", \"label_fine_grained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Original column name sentiment not in the dataset. Current columns in the dataset: ['Unnamed: 0', 'TEXT', 'label']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[353], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Rename the column label_posneg to label [0, 2]\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset_train_eval \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_train_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info/lib/python3.9/site-packages/datasets/dataset_dict.py:391\u001b[0m, in \u001b[0;36mDatasetDict.rename_column\u001b[0;34m(self, original_column_name, new_column_name)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 391\u001b[0m     {\n\u001b[1;32m    392\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mrename_column(original_column_name\u001b[38;5;241m=\u001b[39moriginal_column_name, new_column_name\u001b[38;5;241m=\u001b[39mnew_column_name)\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    394\u001b[0m     }\n\u001b[1;32m    395\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info/lib/python3.9/site-packages/datasets/dataset_dict.py:392\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    391\u001b[0m     {\n\u001b[0;32m--> 392\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_column_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    394\u001b[0m     }\n\u001b[1;32m    395\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info/lib/python3.9/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info/lib/python3.9/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info/lib/python3.9/site-packages/datasets/arrow_dataset.py:2061\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[1;32m   2059\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2062\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2064\u001b[0m     )\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2067\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2068\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2069\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2070\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Original column name sentiment not in the dataset. Current columns in the dataset: ['Unnamed: 0', 'TEXT', 'label']"
     ]
    }
   ],
   "source": [
    "# Rename the column label_posneg to label [0, 2]\n",
    "dataset_train_eval = dataset_train_eval.rename_column(\"sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'TEXT', 'label'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataset_train_eval\n",
    "dataset_train_eval[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'DATE', 'AUTHOR', 'TEXT', 'FAVORITE', 'year', 'manufacturer', 'model', 'Initial Index'],\n",
       "        num_rows: 39347\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-d0480d9a4aaa7bae/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1b3ca36310779346.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/parkhiagarwal/.cache/huggingface/datasets/csv/default-fe9974d2b3317d75/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cde498592c4f1268.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer: Hello, How are You: [Hello, How, Are, You]\n",
    "# Specify Max Length 42 for now, the default model does not have a predefined maximum length\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"TEXT\"], padding=\"max_length\", truncation=True, max_length=42)\n",
    "\n",
    "\n",
    "tokenized_datasets_train = dataset_train_eval['train'].map(tokenize_function, batched=True)\n",
    "tokenized_datasets_eval = dataset_train_eval['test'].map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = test['train'].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We load the model, and specify the num_labels as 3 [pos, neg, neutral]\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics, we can include more such as: F-measure\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import pytorch and check for GPU availability, for using torch.argmax in the future\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to logits and use argmax function, need to change np.argmax to torch.argmax\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average = 'micro')[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average = 'micro')[\"recall\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter training, for now using default hyperparameters\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use optuna to do hyperparameter search\n",
    "# def optuna_hp_space(trial):\n",
    "#    return {\n",
    "#        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "#        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n",
    "#    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_train,\n",
    "    eval_dataset=tokenized_datasets_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parkhiagarwal/opt/anaconda3/envs/info/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='264' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [264/264 07:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.346157</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557495</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.684022</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=264, training_loss=0.2094721505136201, metrics={'train_runtime': 446.2253, 'train_samples_per_second': 4.706, 'train_steps_per_second': 0.592, 'total_flos': 45324990396000.0, 'train_loss': 0.2094721505136201, 'epoch': 3.0})"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6840221285820007,\n",
       " 'eval_accuracy': 0.8933333333333333,\n",
       " 'eval_precision': 0.8933333333333333,\n",
       " 'eval_recall': 0.8933333333333333,\n",
       " 'eval_runtime': 16.4025,\n",
       " 'eval_samples_per_second': 18.29,\n",
       " 'eval_steps_per_second': 2.317,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39347, 2)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a fun car to drive, great gas mileage, cute etc, but very expensive for repairs. In one year, I've spent $2500 out of pocket on a car I bought brand new from the dealer, in addition to the gas to get to the dealership (100 miles roundtrip in DC traffic) and taking time off work (they close at 7pm). Prediction:  0\n",
      "My clutch went out at 14,000 miles, windows finicky (don't work all the time). Re the clutch: no explanation other than its my fault from Mini of North American and the dealer; I had to pay for the replacement less than a year after purchasing it. My previous clutches didn't burn out until 60,000-75,000 miles, so I don't understand this, but I won't buy another mini and will sell this one within the year. Prediction:  0\n",
      "I love my British racing green convertible. The sidewalk package has all the bells and whistles esp the gorgeous brown leather interior. Now they won't make it any more. I am disappointed I wanted a new one only because I think the rear window visibility would be much improved with the retractable roll bars. Prediction:  1\n",
      "Had the car for little over a year now. It is an absolute BLAST to drive, and will always put a smile on your face. Fuel economy is phenomenal for a car this fast as well. Lots of cool features, like the folding mirrors, that are more fun the show of than functional, but fun either way. The pushbutton start is cool, but I wish it had the comfort access system. The leatherette looks nice, but I don't like the way it feels. All in all, for $25,550 and the included maintainance, its a lot of fun for the money. Prediction:  1\n",
      "This is my second burgundy Mini. My first was a 1967 that I brought back from Europe. Both cars were/are go- carts. The '67 cornered better, but was underpowered. The '08 has the power but it overpowers the steering. No complaints, mine you! Lord, what a fun car to drive. By the way, the title refers to a carnival ride, \"The Wild Maus.\" It would snap your head off your shoulders, if you weren't careful. Read the complaints: Bumpy ride: It is a sports car - dodge them. Limited space: drop the back seats and think again. Odd radio controls: It has a radio? - who knew! I love to hear it purr! Prediction:  1\n",
      "OMFG THIS THING IS A BLAST!!! From the very get go I fell in love with a car my wife had suggested when I thought of getting a bug... I'm glad I went with the Mini. First of all I commute 184 Miles a day up and down from the mountains. Phx to Payson AZ of an elevation change of over 5000 ft. I needed something reliable and could handle the road and elements. This little car has sold me for LIFE! I love the Tech with the Sports handling. If you're thinking of buying your wasting precious time you could be driving. =) Prediction:  1\n",
      "So far, my MINI is winner. As all other reviewers have commented on-- it's a blast to drive! Test drove the other subcompacts and MINI stood out from the pack. Cornering and handling are outstanding with no body roll. Braking is very responsive and fuel economy is excellent averaging about 31mpg combined. Getting 44mpg+ on the Hwy with a light foot in 6th gear. Styling is one of a kind. Base Cooper still nicely equipped without a lot of added options. Interior fit and finish is impressive. Ride is a bit firm, but still in a fun kind of way, although this can get tiring on long drives I must admit. Watch the road dips as this car sits low to the ground. Prediction:  1\n",
      "Pros: handling fuel efficiency (my avg 30.4, I think) parking (mwah, ha ha!!!!!...that little space is mine all miiiiiine!) power/acceleration dash layout fit and finish reliability ipod-friendly ($12.00 male-male cord. Plugs right in...totally easy!) Cons: Very jerky acceleration Somewhat jerky braking The seat belt thing gets caught in the space where the door jamb is.I've tweaked my back/shoulder muscles a few times. Hate that! Awful cupholders. If you can get a soda cup out without crushing it, you are amazing and you should become a surgeon. Trunk locks when doors are locked. Even if you've already \"popped\" it. Annoying! Wind wiper controls stink. And only two intermittent speeds. Prediction:  1\n",
      "After 12000 plus miles the car is flawless..it gets better than sticker gas mileage..onboard computer displays your MPG in realtime and average...great feature...not a single problem ..average 36.5-39.5 ..it will get over 40 MPG if you drive it easy on the highway at 70...Sports car ride more firmly than a luxury car..It is incrediblely stable and safe at \"highway plus\" speeds...also has fantastic brakes..Tons of leg room..We plan to add a 2009 S Convertible so our 2008(yellow) will have a playmate...as soon as she decides on the color....If you're a car guy you'll love this car....If you not, you'll still love this car! Prediction:  1\n",
      "O.K., it's cute, fit well in my garage and was truly \"fun to drive\", but the novelty for us was over for us in a big hurry. It is a very small car, and while likely the best in its class, we found it cramped, noisey, rattley and poor riding, especially on road trips. The run-flat tires are bricks, the premium stereo was fair at best, it was nearly impossible to get comfortable (no,we aren't big people), and I really didn't care for the valve rattle at start-up on cold days. I switched to a Camry SE V6 which is (yes) faster, more refined, has far nicer/more of \"everything\", and for about the same money. If you need to drive regularly, long distances or with others, don't buy a Cooper Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(tokenized_dataset_test[\"TEXT\"][x], \"Prediction: \", preds[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "#metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_metric = evaluate.load(\"f1\")\n",
    "#f1_metric.compute(predictions=preds, references=predictions.label_ids, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "21047299851eed5b61a9f73e59545ae031f81177c987d616383d6dc605edc4fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
